# 데이터마이닝 - 2

- 심규석 교수님

## Expectation-Maximization (EM) Clustering

- 몇개의 모수에 대한 초기값을 추정하는 것으로 시작하여 이 모수를 이용해 각각의 데이터가 군집에 속할 확률을 계산한다. 다음으로 계산된 확률을 이용해 모수를 채추정하고 이 과정을 반복하게 된다.

### 생성모델 Generative model

- 준지도학습(semi-supervised learning) 
- class의 분포에 주목하여 분류
- 데이터의 확률분포를 학습 
- Given observable data를 통해 Hidden parameter 도출

### Gaussian Mixture Models (GMM)

- K-Means보다 클러스터 집합이 flexible(분산 덕분에 덜 제한적이라서).
- GMMs는 확률을 사용하기 때문에 각 data point마다 여러 클러스터를 가질 수 있고 만약 data point가 겹쳐진 두 개의 클러스터 사이에 있다 하더라도 우리는 간단하게 확률로 나눠서 판단할 수 있다. 즉, GMMs는 mixed membership[2](https://michigusa-nlp.tistory.com/27#footnote_27_2)을 지원한다.

### Matrix Factorization

- 추천시스템에 사용되는 Collaborative filtering algorithm
-  **Matrix factorization** algorithms work by decomposing the user-item interaction **matrix** into the product of two lower dimensionality rectangular **matrices**.

## 소감

- 공식이 굉장히 복잡하고 일견 이해가 안되는 부분도 많았으나 구글검색과 위키피디아를 통해 개략적인 이해를 했다. 
- 정확히 어떤 방식으로 이루어지는 지는 이해하지 못했지만, 어떤 분야에서 왜 사용되는 지를 이해한 것 같다. 